<!DOCTYPE html>
<html lang="en">

<head>
    <title>SISR</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="./css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="./css/navbar-fixed.css">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
</head>

<body style="background-color: white; font-family: Open Sans, sans-serif;">
    <nav class="navbar navbar-expand-md border-bottom fixed-top bg-light fw-medium">
        <div class="container-fluid">
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <ul class="navbar-nav me-auto mb-2 mb-md-0">
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" data-bs-toggle="dropdown"
                            aria-expanded="false">Demo</a>
                        <ul class="dropdown-menu">
                            <li><a class="dropdown-item" href="../demo/index.html">Main task</a></li>
                            <li><a class="dropdown-item" href="../additional_demo/index.html">Additional task</a></li>
                        </ul>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../example/index.html">Examples</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" aria-current="page" href="#">Details</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="../comparison/index.html">Comparison</a>
                    </li>
                    <li class="nav-item ">
                        <a class="nav-link" href="../about_us/index.html">About Us</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>


    <main class="container">
        <div class="container px-4 py-5">
            <h2 class="pb-2 fw-bold border-bottom">Network Architecture</h2>
            <p class="text-body-secondary">Reference to <a href="https://arxiv.org/abs/2205.04437"
                    target="_blank">Activating More Pixels in Image Super-Resolution Transformer</a>
            </p>
            <caption class="text-body-secondary px-2">
                <ul class="list-unstyled">
                    <li><strong>The Overall Structure</strong></li>
                    <li> The overall network consists of three parts, including shallow feature extraction, deep feature
                        extraction and image reconstruction. Specifically:</li>
                    <ul>
                        <li>
                            For a given low-resolution (LR) input \( I_{LR} \in \mathbb{R}^{H \times W \times C_{in}}
                            \), we first exploit one convolution layer to extract the shallow feature \( F_0 \in
                            \mathbb{R}^{H \times W \times C} \)
                            <br> where \( C_{in} \) and \( C \) denote the channel number of the input and the
                            intermediate feature.
                        </li>
                        <li>
                            Then, a series of residual hybrid attention groups (RHAG) and one 3 \( \times \) 3
                            convolution layer \( H_{Conv}(\cdot) \) are utilized to perform the deep feature extraction.
                        </li>
                        <li>
                            After that, we add a global residual connection to fuse shallow features \( F_0 \) and deep
                            features \( F_D \in
                            \mathbb{R}^{H \times W \times C} \)
                        </li>
                        <li>
                            Finally, reconstruct the high-resolution result via a reconstruction module.
                        </li>
                    </ul>
                    <li>
                        As depicted in figure below, each RHAG contains several hybrid attention blocks (HAB), an
                        overlapping cross-attention block (OCAB) and a 3 \( \times \) 3 convolution layer with a
                        residual connection. For the reconstruction module, the pixel-shuffle method is adopted to
                        up-sample the fused feature.
                    </li>
                </ul>

            </caption>
            <img class="card mw-100" src="./images/Network.png">
        </div>

        <div class="container px-4 py-5">
            <h2 class="pb-2 fw-bold border-bottom">Training settings</h2>

            <caption class="text-body-secondary px-2">
                <ul>
                    <li><strong>Optimizer</strong></li>
                    <ul class="list-unstyled">
                        <li>
                            We adopt <em>Adam</em> optimizer to train the model. It is is a stochastic gradient descent
                            extension
                            that updates network weights during training. It is a hybrid of the “gradient descent with
                            momentum” and the “RMSP” algorithms.
                        </li>
                        <ul>
                            <li>
                                The learning rate is initialized as \( 1 \times e^{-4} \)
                            </li>
                            <li>
                                \( \beta_1 \) is initialized as \( 0.9 \)
                            </li>
                            <li>
                                \( \beta_2 \) is initialized as \( 0.99 \)
                            </li>
                        </ul>
                    </ul>
                    <br>
                    <li><strong>Scheduler</strong></li>
                    <ul class="list-unstyled">
                        <ul>
                            <li>
                                The Scheduler type is \( MultiStepLR \)
                            </li>
                            <li>
                                The \( milestones \) initiated are \( [125000, 200000, 225000, 250000] \)
                            </li>
                            <li>
                                \( gamma \) is initialized as \( 0.99 \)
                            </li>
                        </ul>
                        <li>
                            This means that the scheduler will reduce the learning rate by half at iteration milestones
                            \( [125000, 200000, 225000, 250000] \)
                        </li>
                    </ul>
                    <br>
                    <li><strong>Loss Functions</strong></li>
                    <ul class="list-unstyled">
                        <li>
                            We use a total of 3 different loss functions:
                        </li>
                        <ul>
                            <li>
                                <em>L1 Loss:</em>
                                <br>
                                It also called Mean Absolute Error (MAE), computes the average of the sum of absolute
                                differences between actual values and predicted values.
                                <br>
                                \[ loss_{L1}(Y, Y') = \frac{1}{mn} \sum_{i=1}^{m} \sum_{i=1}^{n} \left| Y(i, j) - Y'(i,
                                j)
                                \right| \]
                                <br>
                                Where, \( Y \) represents the matrix data of ground truth image.
                                \( Y' \) represents the matrix data of output image.
                                \( m \) represents the numbers of rows of pixels and
                                \( i \) represents the index of that row of the image.
                                \( n \) represents the number of columns of pixels and
                                \( j \) represents the index of that column of the image.
                            </li>
                            <li>
                                <em>Perceptual Loss:</em>
                                <br>
                                It is designed to capture perceptual differences between images, such as content and
                                style discrepancies, which are not always evident at the pixel level.
                                <br>
                                Here, vgg19 network used as feature extractor and the conv2_2 feature layer will be
                                extracted in calculating losses. Criterion used for perceptual loss is MAE.
                                <br>
                                \[ loss_{Perceptual}(Y, Y') = \frac{1}{n} \sum_{i=1}^{n} \left| feature_i(Y) -
                                feature_i(Y') \right| \]
                                <br>
                                Where, \( Y \) represents the matrix data of ground truth image.
                                \( Y' \) represents the matrix data of output image.
                                \( n \) represents the numbers of features in conv2_2 layer and
                                \( i \) represents the index of that feature of the conv2_2 layer.
                            </li>
                            <li>
                                <em>MixLosses Loss:</em>
                                <br>
                                It is designed to combine both the characteristics of <em>L1 Loss</em> and
                                <em>Perceptual Loss</em>,
                                specifically balancing differences in color pixels between images and perceptual
                                differences between images.
                                <br>
                                \[ loss_{MixLosses}(Y, Y') = \alpha \times loss_{L1}(Y, Y') + \beta \times
                                loss_{Perceptual}(Y, Y') \]
                                <br>
                                Where, \( Y \) represents the matrix data of ground truth image.
                                \( Y' \) represents the matrix data of output image.
                                \( \alpha \) represents the L1 loss weight and
                                \( \beta \) represents the Perceptual loss weight.
                            </li>
                        </ul>
                    </ul>
                    <br>
                    <li><strong>Others</strong></li>
                    <ul class="list-unstyled">
                        <ul>
                            <li>
                                Total iteration is initialized as \( 250000 \)
                            </li>
                            <li>
                                Dataloader batch size is initiated as \( 4 \)
                            </li>
                        </ul>
                        <li>
                            This means that each iteration will process a batch of 4 training images and finish training
                            the model at the 250,000th iteration.
                        </li>
                    </ul>
                </ul>

            </caption>
        </div>

        <div class="container px-4 py-5">
            <h2 class="pb-2 fw-bold border-bottom">Metrics</h2>
            <caption class="text-body-secondary px-2">
                <ul class="list-unstyled">
                    <li>We use 2 metrics to evaluate models, including:</li>
                    <ul>
                        <li>
                            <em>Peak Signal-to-Noise Ratio (PSNR):</em>
                            <br>
                            It is the ratio between the maximum possible power of an image and the power of corrupting
                            noise
                            that affects the quality of its representation. To estimate the PSNR of an image, it is
                            necessary to compare that image to an ideal clean image with the maximum possible power.
                            <br>
                            PSNR is defined as follows:
                            <br>
                            \[PSNR = 10 \cdot \log_{10} \frac{{(L - 1)^2}}{{MSE}} \]
                            <br>
                            Here, \( L \) is the maximum possible number of intensity levels (minimum
                            intensity level assumed to
                            be 0) in an image. In this project, the image data we use are all 8-bit color images, so L
                            has a
                            value of 256. \( MSE \) is the mean squared error and it is defined as:
                            <br>
                            \[MSE = \frac{1}{mn} \sum_{i=1}^{m} \sum_{i=1}^{n} (Y'(i, j) - Y(i, j))^2 \]
                            <br>
                            Where, \( Y \) represents the matrix data of ground truth image.
                            \( Y' \) represents the matrix data of
                            output image. \( m \) represents the numbers of rows of pixels and
                            \( i \) represents the index of that row
                            of the image. \( n \) represents the number of columns of pixels and
                            \( j \) represents the index of that
                            column of the image.
                        </li>
                        <br>
                        <li>
                            <em>Structured Similarity Index Measurement (SSIM): </em>
                            <br>
                            It is a metric of comparison to check the similarity between the original image and
                            output-image. It measures the perceptual difference between the two images.
                            <br>
                            SSIM is defined as follows:
                            <br>
                            \[SSIM(P_{Y^*}, P_Y) = \frac{(2\mu_{P_{Y^*}} \mu_{P_Y} + c_1)(2\sigma_{P_{Y^*}} \sigma_{P_Y}
                            +
                            c_2)}{(\mu_{P_{Y^*}}^2 + \mu_{P_Y}^2 + c_1)(\sigma_{P_{Y^*}}^2 + \sigma_{P_Y}^2 + c_2)} \]
                            <br>
                            Where, \( Y \) represents the matrix data of ground truth image.
                            \( Y' \) represents the matrix data of output image. 
                            \( \mu \) represents the knowledge of \( P \) patchs and
                            \( \sigma \) represents the standard deviation of \( P \) patchs. 
                            \( c_1 \) and \( c_2 \) are minor contants.
                        </li>
                    </ul>
                </ul>
            </caption>
        </div>


        <footer class="d-flex flex-wrap justify-content-between align-items-center py-3 my-4 border-top">
            <div class="col-md-4 d-flex align-items-center">
                <a href="/" class="mb-3 me-2 mb-md-0 text-body-secondary text-decoration-none lh-1">
                    <svg class="bi" width="30" height="24">
                        <use xlink:href="#bootstrap"></use>
                    </svg>
                </a>
                <span class="mb-3 mb-md-0 text-body-secondary">Group GFA23AI17 - project FA23AI08</span>
            </div>

            <ul class="nav col-md-4 justify-content-end list-unstyled d-flex">
                <li class="ms-3"><a class="text-body-secondary"
                        href="https://github.com/ngoduythinh250601/HAT-experiment.git" target="_blank"><svg class="bi"
                            width="24" height="24">
                            <use xlink:href="#github"></use>
                        </svg></a></li>
                <li class="ms-3"><a class="text-body-secondary"
                        href="https://drive.google.com/drive/folders/1rtrWZ_BP1QOc4smIZqenU_G7Uro3pOQW?usp=sharing"
                        target="_blank"><svg class="bi" width="24" height="24">
                            <use xlink:href="#drive"></use>
                        </svg></a></li>
                <li class="ms-3"><a class="text-body-secondary" href="https://www.facebook.com/thinhduy2506 "
                        target="_blank"><svg class="bi" width="24" height="24">
                            <use xlink:href="#facebook"></use>
                        </svg></a></li>
            </ul>
        </footer>
        <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
            <symbol id="bootstrap" viewBox="0 0 118 94">
                <title>Bootstrap</title>
                <path fill-rule="evenodd" clip-rule="evenodd"
                    d="M24.509 0c-6.733 0-11.715 5.893-11.492 12.284.214 6.14-.064 14.092-2.066 20.577C8.943 39.365 5.547 43.485 0 44.014v5.972c5.547.529 8.943 4.649 10.951 11.153 2.002 6.485 2.28 14.437 2.066 20.577C12.794 88.106 17.776 94 24.51 94H93.5c6.733 0 11.714-5.893 11.491-12.284-.214-6.14.064-14.092 2.066-20.577 2.009-6.504 5.396-10.624 10.943-11.153v-5.972c-5.547-.529-8.934-4.649-10.943-11.153-2.002-6.484-2.28-14.437-2.066-20.577C105.214 5.894 100.233 0 93.5 0H24.508zM80 57.863C80 66.663 73.436 72 62.543 72H44a2 2 0 01-2-2V24a2 2 0 012-2h18.437c9.083 0 15.044 4.92 15.044 12.474 0 5.302-4.01 10.049-9.119 10.88v.277C75.317 46.394 80 51.21 80 57.863zM60.521 28.34H49.948v14.934h8.905c6.884 0 10.68-2.772 10.68-7.727 0-4.643-3.264-7.207-9.012-7.207zM49.948 49.2v16.458H60.91c7.167 0 10.964-2.876 10.964-8.281 0-5.406-3.903-8.178-11.425-8.178H49.948z">
                </path>
            </symbol>
            <symbol id="facebook" viewBox="0 0 16 16">
                <path
                    d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z" />

            </symbol>
            <symbol id="drive" viewBox="0 0 16 16" alt="drive">
                <path d="M4.5 11a.5.5 0 1 0 0-1 .5.5 0 0 0 0 1zM3 10.5a.5.5 0 1 1-1 0 .5.5 0 0 1 1 0z" />
                <path
                    d="M16 11a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V9.51c0-.418.105-.83.305-1.197l2.472-4.531A1.5 1.5 0 0 1 4.094 3h7.812a1.5 1.5 0 0 1 1.317.782l2.472 4.53c.2.368.305.78.305 1.198V11zM3.655 4.26 1.592 8.043C1.724 8.014 1.86 8 2 8h12c.14 0 .276.014.408.042L12.345 4.26a.5.5 0 0 0-.439-.26H4.094a.5.5 0 0 0-.44.26zM1 10v1a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1v-1a1 1 0 0 0-1-1H2a1 1 0 0 0-1 1z" />

            </symbol>
            <symbol id="github" viewBox="0 0 16 16" alt="github">
                <path
                    d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" />
            </symbol>
        </svg>
    </main>
    <script src="./js/bootstrap.bundle.min.js"></script>
</body>

</html>